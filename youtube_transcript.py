# -*- coding: utf-8 -*-
"""youtube-transcript.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19riSAAMek4qBIQQ0HZVb-H2h505esaHW
"""

!pip install -q git+https://github.com/huggingface/transformers
!pip install -qU langchain Faiss-gpu tiktoken sentence-transformers
!pip install -qU trl Py7zr auto-gptq optimum
!pip install -q rank_bm25
!pip install -q PyPdf

import langchain
from langchain.embeddings import CacheBackedEmbeddings,HuggingFaceEmbeddings
from langchain.vectorstores import FAISS
from langchain.storage import LocalFileStore
from langchain.retrievers import BM25Retriever,EnsembleRetriever
from langchain.document_loaders import PyPDFLoader,DirectoryLoader
from langchain.llms import HuggingFacePipeline
from langchain.cache import InMemoryCache
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import prompt
from langchain.chains import RetrievalQA
from langchain.callbacks import StdOutCallbackHandler
from langchain import PromptTemplate
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline

!pip install yt_dlp
!pip install openai

!pip install chromadb

!pip install youtube-transcript-api

from langchain.document_loaders import YoutubeLoader

loader = YoutubeLoader.from_youtube_url(
    "https://www.youtube.com/watch?v=QsYGlZkevEg", add_video_info=True
)

!pip install pytube

transcript = loader.load()

transcript[0]

from langchain.text_splitter import RecursiveCharacterTextSplitter,CharacterTextSplitter

chunk_size=26
chunk_overlap=4

r_splitter=RecursiveCharacterTextSplitter(
  chunk_size=chunk_size,
  chunk_overlap=chunk_overlap,
)

c_splitter=CharacterTextSplitter(
  chunk_size=chunk_size,
  chunk_overlap=chunk_overlap,
)



text_splitter=CharacterTextSplitter(
  chunk_size=1000,
  chunk_overlap=150,
  length_function=len,
  separator=' ',
)

docs=text_splitter.split_documents(transcript)

print(len(docs))
len(transcript)

# Function to pretty print text
from IPython.display import display, Markdown

def pretty_print_paragraph(text):
    # Split the text into paragraphs (assuming paragraphs are separated by double newlines)
    paragraphs = text.split('\n\n')

    # Create a Markdown string with line breaks between paragraphs
    formatted_text = '\n\n'.join(paragraphs)

    # Display the formatted text using Markdown
    display(Markdown(formatted_text))

pretty_print_paragraph(docs[0].page_content[:])

! pip install transformers

import requests

API_URL = "https://api-inference.huggingface.co/models/mistralai/Mistral-7B-v0.1"
headers = {"Authorization": "Bearer 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"}

def query(payload):
	response = requests.post(API_URL, headers=headers, json=payload)
	return response.json()

output = query({
	"inputs": "Can you please let us know more details about your ",
})

from transformers import pipeline
summarizer = pipeline('summarization')

import requests

# Set up your Mistral7B API key
api_key = "https://api-inference.huggingface.co/models/mistralai/Mistral-7B-v0.1"  # Replace with your actual Mistral7B API key

# Define the input text to be summarized
input_text = "Your input text goes here."

# Define the API endpoint and parameters
endpoint = "https://api.mistral7.com/summarize"
params = {
    "text": input_text,
    "max_length": 30  # You can adjust the length of the summary
}

# Set the API key in the headers
headers = {
    "Authorization": f"Bearer {api_key}"
}

# Send a POST request to the Mistral7B API
response = requests.post(endpoint, params=params, headers=headers)

# Check if the request was successful
if response.status_code == 200:
    summary = response.json()["summary"].strip()
    print(f"Summary: {summary}")
else:
    print(f"Error: {response.status_code}")

# # loaders=[
# #   PyPDFLoader("/content/machinelearning-lecture01.pdf"),
# #   PyPDFLoader("/content/MachineLearning-Lecture02.pdf"),
# #   PyPDFLoader("/content/MachineLearning-Lecture03.pdf"),
# # ]

# docs=[]
# for loader in transcript:
#   docs.extend(loader.load())

# print(len(docs))

# Split the text
splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)

split_docs=splitter.transform_documents(docs)

print(len(split_docs))

# We will use a local file system for storing embeddings and FAISS vector store for retrieval
store=LocalFileStore("./cache")

# Now we will create the embeddings for the documents and the vector store will be FAISS using HuggingFaceEmbeddings
embed_model='BAAI/bge-small-en-v1.5'
core_embeddings_model=HuggingFaceEmbeddings(model_name=embed_model)
embedder=CacheBackedEmbeddings.from_bytes_store(core_embeddings_model, store,namespace=embed_model)

# Create the vector store
vectorstore=FAISS.from_documents(transcript,embedder)

bm25_retriever=BM25Retriever.from_documents(transcript)
bm25_retriever.k=5

query="What did they say about a show called 'the last of us'?"
embedding_vector=core_embeddings_model.embed_query(query)
print(len(embedding_vector))

docs_resp=vectorstore.similarity_search_by_vector(embedding_vector,k=5)

for page in docs_resp:
  print(page.page_content)
  print("\n")

# Setup the ensemble Retriever
faiss_retriever=vectorstore.as_retriever(search_kwargs={"k":5})
ensemble_retriever=EnsembleRetriever(retrievers=[bm25_retriever,faiss_retriever],weights=[0.5,0.5])

model_name="TheBloke/Mistral-7B-Instruct-v0.1-GPTQ"
model=AutoModelForCausalLM.from_pretrained(model_name,device_map="auto",trust_remote_code=False,revision="gptq-8bit-32g-actorder_True")

tokenizer=AutoTokenizer.from_pretrained(model_name,use_fast=True)

# Create the pipeline
pipe=pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    max_new_tokens=1024,
    do_sample=True,
    temperature=0.1,
    top_p=0.95,
    top_k=40,
    repetition_penalty=1.1,


)

!pip install langchain

from langchain.llms import HuggingFacePipeline
llm=HuggingFacePipeline(pipeline=pipe)

langchain.llm_cache=InMemoryCache()

PROMPT_TEMPLATE = """
You are my study companion who helps me during my study hours.You are very good at providing summarization and preparing test questions using the data provided.
With the information being provided try to answer the question.
If you cant answer the question based on the information either say you cant find an answer or unable to find an answer.
So try to understand in depth about the context and answer only based on the information provided. Dont generate irrelevant answers

Context: {context}
Question: {question}
Do provide only helpful answers

Helpful answer:
"""

input_variables=['context','question']

custom_prompt=PromptTemplate(input_variables=input_variables,template=PROMPT_TEMPLATE)

# Setup retrieval chain without Hybrid Search
handler=StdOutCallbackHandler()

qa_with_sources_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever = vectorstore.as_retriever(search_kwargs={"k":5}),
    verbose=True,
    callbacks=[handler],
    chain_type_kwargs={"prompt": custom_prompt},
    return_source_documents=True
)

query="What did they say about regression in the first lecture Explain it in 1500 words"
response=qa_with_sources_chain({"query":query})
print(f"Response generated : \n {response['result']}\n")
print(f"Source Documents : \n {response['source_documents']}\n")

print(len(response['result']))

# Setup retrieval chain with Hybrid Search
handler=StdOutCallbackHandler()
qa_chain_hybrid_search = RetrievalQA.from_chain_type(
  llm=llm,
  chain_type="stuff",
  retriever = ensemble_retriever,
  callbacks=[handler],
  chain_type_kwargs={"prompt": custom_prompt},
  return_source_documents=True,
)

# Now lets give the same query as above
response=qa_chain_hybrid_search({"query":query})
print(f"Response generated : \n {response['result']}\n\n")
print(f"Source Documents : \n {response['source_documents']}")

query="Could you tell me if probablity is a topic in the given lectures?"
response=qa_chain_hybrid_search({"query":query})
print(f"Response generated : \n {response['result']}\n\n")
print(f"Source Documents : \n {response['source_documents']}")

query="What are the topics covered in the first lecture?"
response=qa_chain_hybrid_search({"query":query})
print(f"Response generated : \n {response['result']}\n\n")

query="What did they talk about Matlab in the first lecture?"
response=qa_chain_hybrid_search({"query":query})
print(f"Response generated : \n {response['result']}\n\n")
print(f"Source Documents : \n {response['source_documents']}")

query="Give me a summary of all the lectures and explain each summary briefly and give me important points from each lecture"
response=qa_chain_hybrid_search({"query":query})
print(f"Response generated : \n {response['result']}\n\n")
print(f"Source Documents : \n {response['source_documents']}")

query="Frame some questions about the first lecture that might be confusing and ask only relavent questions"
response=qa_chain_hybrid_search({"query":query})
print(f"Response generated : \n {response['result']}\n\n")
print(f"Source Documents : \n {response['source_documents']}")

